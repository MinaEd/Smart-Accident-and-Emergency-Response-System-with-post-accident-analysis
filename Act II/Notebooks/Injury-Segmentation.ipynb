{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10601917,"sourceType":"datasetVersion","datasetId":6562522},{"sourceId":10603737,"sourceType":"datasetVersion","datasetId":6563831},{"sourceId":10604015,"sourceType":"datasetVersion","datasetId":6564040},{"sourceId":10604021,"sourceType":"datasetVersion","datasetId":6564044},{"sourceId":10612683,"sourceType":"datasetVersion","datasetId":6570246},{"sourceId":10630082,"sourceType":"datasetVersion","datasetId":6581769}],"dockerImageVersionId":30840,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"colab":{"provenance":[]}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# First PART","metadata":{"id":"hEPmgSLiNePI"}},{"cell_type":"markdown","source":"# WORKING MULTICLASS SEGMENTATION\n## START\n","metadata":{"id":"G6zBxW9yNePO"}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras import layers, Model\nimport tensorflow.keras.backend as K\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Constants\nIMG_SIZE = (256, 256)\nBATCH_SIZE = 32\nEPOCHS = 5\n#First, update the number of classes to include background\nNUM_CLASSES = 6  # 5 injury types + 1 background class\n\n#Update INJURY_CLASSES dictionary to include background\nINJURY_CLASSES = {\n    0: {\"name\": \"background\", \"color\": (0, 0, 0)},      # Black for background\n    1: {\"name\": \"abrasion\", \"color\": (255, 0, 0)},      # Red\n    2: {\"name\": \"bruise\", \"color\": (0, 0, 255)},        # Blue\n    3: {\"name\": \"cut\", \"color\": (0, 255, 0)},           # Green\n    4: {\"name\": \"laceration\", \"color\": (255, 255, 0)},  # Yellow\n    5: {\"name\": \"stab_wound\", \"color\": (255, 0, 255)}   # Magenta\n}\n\n# Define paths\ntrain_files = [\"/kaggle/input/compound-injury-dataset/train.tfrecord\"]\nval_files = [\"/kaggle/input/compound-injury-dataset/val.tfrecord\"]\ntest_files = [\"/kaggle/input/compound-injury-dataset/test.tfrecord\"]\n\ndef weighted_categorical_accuracy(y_true, y_pred):\n    \"\"\"\n    Custom accuracy metric that handles class imbalance\n    by weighting each class equally regardless of pixel count\n    \"\"\"\n    threshold = 0.5\n    accuracies = []\n\n    for i in range(NUM_CLASSES):\n        y_true_class = y_true[..., i]\n        y_pred_class = tf.cast(y_pred[..., i] > threshold, tf.float32)\n\n        true_positives = K.sum(y_true_class * y_pred_class)\n        total_pixels = K.sum(y_true_class)\n\n        # Avoid division by zero\n        class_accuracy = tf.where(\n            total_pixels > 0,\n            true_positives / (total_pixels + K.epsilon()),\n            0.0\n        )\n        accuracies.append(class_accuracy)\n\n    return K.mean(tf.stack(accuracies))\n\n\ndef parse_tfrecord(example_proto, img_size=IMG_SIZE, training=False):\n    \"\"\"\n    Parse TFRecord format data into images and masks with proper background handling\n    \"\"\"\n    feature_description = {\n        'image/encoded': tf.io.FixedLenFeature([], tf.string),\n        'image/height': tf.io.FixedLenFeature([], tf.int64),\n        'image/width': tf.io.FixedLenFeature([], tf.int64),\n        'image/object/bbox/xmin': tf.io.VarLenFeature(tf.float32),\n        'image/object/bbox/xmax': tf.io.VarLenFeature(tf.float32),\n        'image/object/bbox/ymin': tf.io.VarLenFeature(tf.float32),\n        'image/object/bbox/ymax': tf.io.VarLenFeature(tf.float32),\n        'image/object/class/label': tf.io.VarLenFeature(tf.int64),\n    }\n\n    example = tf.io.parse_single_example(example_proto, feature_description)\n\n    # Decode and preprocess image\n    image = tf.io.decode_jpeg(example['image/encoded'], channels=3)\n    image = tf.image.resize(image, img_size)\n    image = tf.cast(image, tf.float32) / 255.0\n\n    # if training:\n    #     image = tf.image.random_brightness(image, 0.1)\n    #     image = tf.image.random_contrast(image, 0.9, 1.1)\n    #     image = tf.clip_by_value(image, 0, 1)\n\n    # Get bounding box coordinates and class labels\n    xmin = tf.sparse.to_dense(example['image/object/bbox/xmin'])\n    xmax = tf.sparse.to_dense(example['image/object/bbox/xmax'])\n    ymin = tf.sparse.to_dense(example['image/object/bbox/ymin'])\n    ymax = tf.sparse.to_dense(example['image/object/bbox/ymax'])\n    class_labels = tf.cast(tf.sparse.to_dense(example['image/object/class/label']), tf.int32)\n\n    # Initialize masks tensor with -1 to distinguish unclassified pixels\n    masks = tf.fill(img_size, -1)\n\n    def process_box(i, masks):\n        y_start = tf.cast(ymin[i] * tf.cast(img_size[0], tf.float32), tf.int32)\n        y_end = tf.cast(ymax[i] * tf.cast(img_size[0], tf.float32), tf.int32)\n        x_start = tf.cast(xmin[i] * tf.cast(img_size[1], tf.float32), tf.int32)\n        x_end = tf.cast(xmax[i] * tf.cast(img_size[1], tf.float32), tf.int32)\n\n        y_coords = tf.range(y_start, y_end)\n        x_coords = tf.range(x_start, x_end)\n        y_coords, x_coords = tf.meshgrid(y_coords, x_coords, indexing='ij')\n\n        # Shift class indices by 1 to account for background class\n        class_idx = class_labels[i]\n        indices = tf.stack([\n            tf.reshape(y_coords, [-1]),\n            tf.reshape(x_coords, [-1])\n        ], axis=1)\n        updates = tf.fill([tf.size(y_coords)], class_idx)\n        return tf.tensor_scatter_nd_update(masks, indices, updates)\n\n    # Process each bounding box\n    num_boxes = tf.shape(xmin)[0]\n    i = tf.constant(0)\n\n    def condition(i, masks):\n        return tf.less(i, num_boxes)\n\n    def body(i, masks):\n        masks = process_box(i, masks)\n        return i + 1, masks\n\n    _, final_masks = tf.while_loop(\n        condition,\n        body,\n        [i, masks],\n        shape_invariants=[i.get_shape(), tf.TensorShape(img_size)]\n    )\n\n    # Set unclassified pixels (still -1) to background class (0)\n    final_masks = tf.where(final_masks < 0, 0, final_masks)\n    \n    # Convert to one-hot encoding\n    final_masks = tf.one_hot(final_masks, NUM_CLASSES)\n    \n    return image, final_masks\n\ndef multiclass_dice_loss(y_true, y_pred):\n    \"\"\"\n    Simplified Dice loss for multi-class segmentation with proper gradient computation\n    \"\"\"\n    smooth = 1.0\n    loss = 0\n    \n    for i in range(NUM_CLASSES):\n        # Extract the current class\n        y_true_f = y_true[..., i]\n        y_pred_f = y_pred[..., i]\n        \n        # Calculate intersection and union directly\n        intersection = K.sum(y_true_f * y_pred_f, axis=[1, 2])\n        denominator = K.sum(y_true_f, axis=[1, 2]) + K.sum(y_pred_f, axis=[1, 2])\n        \n        # Calculate dice coefficient\n        dice = (2. * intersection + smooth) / (denominator + smooth)\n        loss += (1 - K.mean(dice))\n    \n    return loss / NUM_CLASSES\n\n\ndef multiclass_iou_metric(y_true, y_pred):\n    \"\"\"\n    Simplified IoU metric with proper gradient computation\n    \"\"\"\n    smooth = 1.0\n    ious = []\n    \n    for i in range(NUM_CLASSES):\n        y_true_f = y_true[..., i]\n        y_pred_f = y_pred[..., i]\n        \n        intersection = K.sum(y_true_f * y_pred_f, axis=[1, 2])\n        union = K.sum(y_true_f, axis=[1, 2]) + K.sum(y_pred_f, axis=[1, 2]) - intersection\n        iou = K.mean((intersection + smooth) / (union + smooth))\n        ious.append(iou)\n    \n    return K.mean(tf.stack(ious))\n\ndef load_dataset(tfrecord_files, img_size=IMG_SIZE, batch_size=BATCH_SIZE, training=False):\n    \"\"\"\n    Load and prepare dataset from TFRecord files\n    \"\"\"\n    dataset = tf.data.TFRecordDataset(tfrecord_files)\n    dataset = dataset.map(\n        lambda x: parse_tfrecord(x, img_size, training),\n        num_parallel_calls=tf.data.AUTOTUNE\n    )\n    if training:\n        dataset = dataset.shuffle(1000)\n    dataset = dataset.repeat()\n    dataset = dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n    return dataset\n\ndef enhanced_unet_model(img_size=(256,256),filters_base=64, dropout_rate=0.3):\n    \"\"\"\n    Improved U-Net architecture with:\n    1. Batch Normalization for better training stability\n    2. Spatial Dropout for regularization\n    3. Residual connections in each block\n    4. LeakyReLU activation for better gradient flow\n    5. Optional SE (Squeeze-and-Excitation) blocks for channel attention\n    \"\"\"\n    inputs = layers.Input(shape=(img_size[0], img_size[1], 3))\n\n    def conv_block(x, filters, dropout=True):\n        # First conv layer in block\n        conv = layers.Conv2D(filters, 3, padding='same')(x)\n        conv = layers.BatchNormalization()(conv)\n        conv = layers.LeakyReLU(0.2)(conv)\n\n        # Second conv layer in block\n        conv = layers.Conv2D(filters, 3, padding='same')(conv)\n        conv = layers.BatchNormalization()(conv)\n        conv = layers.LeakyReLU(0.2)(conv)\n\n        # Optional dropout\n        if dropout:\n            conv = layers.SpatialDropout2D(dropout_rate)(conv)\n\n        # Residual connection\n        if x.shape[-1] == filters:\n            conv = layers.Add()([conv, x])\n\n        return conv\n\n    def se_block(x, filters):\n        \"\"\"Squeeze-and-Excitation block for channel attention\"\"\"\n        se = layers.GlobalAveragePooling2D()(x)\n        se = layers.Dense(filters // 16, activation='relu')(se)\n        se = layers.Dense(filters, activation='sigmoid')(se)\n        se = layers.Reshape((1, 1, filters))(se)\n        return layers.multiply([x, se])\n\n    # Encoder\n    conv1 = conv_block(inputs, filters_base, dropout=False)\n    pool1 = layers.MaxPooling2D(2)(conv1)\n\n    conv2 = conv_block(pool1, filters_base*2)\n    pool2 = layers.MaxPooling2D(2)(conv2)\n\n    conv3 = conv_block(pool2, filters_base*4)\n    conv3 = se_block(conv3, filters_base*4)  # Add SE block\n    pool3 = layers.MaxPooling2D(2)(conv3)\n\n    # Bridge\n    conv4 = conv_block(pool3, filters_base*8)\n    conv4 = se_block(conv4, filters_base*8)  # Add SE block\n\n    # Decoder\n    up3 = layers.UpSampling2D(2)(conv4)\n    # Add 1x1 conv for channel matching if needed\n    if up3.shape[-1] != conv3.shape[-1]:\n        up3 = layers.Conv2D(conv3.shape[-1], 1, padding='same')(up3)\n    merge3 = layers.Concatenate()([up3, conv3])\n    conv5 = conv_block(merge3, filters_base*4)\n\n    up2 = layers.UpSampling2D(2)(conv5)\n    if up2.shape[-1] != conv2.shape[-1]:\n        up2 = layers.Conv2D(conv2.shape[-1], 1, padding='same')(up2)\n    merge2 = layers.Concatenate()([up2, conv2])\n    conv6 = conv_block(merge2, filters_base*2)\n\n    up1 = layers.UpSampling2D(2)(conv6)\n    if up1.shape[-1] != conv1.shape[-1]:\n        up1 = layers.Conv2D(conv1.shape[-1], 1, padding='same')(up1)\n    merge1 = layers.Concatenate()([up1, conv1])\n    conv7 = conv_block(merge1, filters_base, dropout=False)\n\n    # Output\n    outputs = layers.Conv2D(NUM_CLASSES, 1, activation='softmax')(conv7)\n\n\n\n    return Model(inputs, outputs)\n\ndef plot_training_history(history):\n    \"\"\"\n    Plot training metrics\n    \"\"\"\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n\n    # Plot loss\n    ax1.plot(history.history['loss'], label='Training Loss')\n    ax1.plot(history.history['val_loss'], label='Validation Loss')\n    ax1.set_title('Model Loss')\n    ax1.set_xlabel('Epoch')\n    ax1.set_ylabel('Loss')\n    ax1.legend()\n\n    # Plot IoU\n    ax2.plot(history.history['multiclass_iou_metric'], label='Training IoU')\n    ax2.plot(history.history['val_multiclass_iou_metric'], label='Validation IoU')\n    ax2.set_title('Model IoU')\n    ax2.set_xlabel('Epoch')\n    ax2.set_ylabel('IoU')\n    ax2.legend()\n\n    plt.tight_layout()\n    plt.show()\n\n\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-02T20:58:37.721657Z","iopub.execute_input":"2025-02-02T20:58:37.722023Z","iopub.status.idle":"2025-02-02T20:58:37.748587Z","shell.execute_reply.started":"2025-02-02T20:58:37.721997Z","shell.execute_reply":"2025-02-02T20:58:37.747673Z"},"id":"Fxq3DbtqNePP"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load datasets\ntrain_dataset = load_dataset(train_files, training=True)\nval_dataset = load_dataset(val_files)\n# Create and compile model\nmultiClassWorkingModel = enhanced_unet_model()\nmultiClassWorkingModel.compile(\n    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n    loss='categorical_crossentropy',  # Changed to standard categorical crossentropy\n    metrics=['categorical_accuracy', multiclass_iou_metric]\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-02T20:58:44.85745Z","iopub.execute_input":"2025-02-02T20:58:44.857814Z","iopub.status.idle":"2025-02-02T20:58:45.61736Z","shell.execute_reply.started":"2025-02-02T20:58:44.857788Z","shell.execute_reply":"2025-02-02T20:58:45.616684Z"},"id":"pClAwT-WNePQ"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#multiClassWorkingModel.summary()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-02T20:21:41.594413Z","iopub.execute_input":"2025-02-02T20:21:41.594743Z","iopub.status.idle":"2025-02-02T20:21:41.66841Z","shell.execute_reply.started":"2025-02-02T20:21:41.594719Z","shell.execute_reply":"2025-02-02T20:21:41.667498Z"},"id":"wq7zau7hNePQ","outputId":"5c9e3e7f-4c98-4313-fbae-a5cf84d67a2c","collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Calculate steps per epoch\nsteps_per_epoch = 2560 // BATCH_SIZE\nvalidation_steps = 190 // BATCH_SIZE\ntest_steps = 120 // BATCH_SIZE\n\n# Train model\nhistory = multiClassWorkingModel.fit(\n    train_dataset,\n    validation_data=val_dataset,\n    epochs=EPOCHS,\n    steps_per_epoch=steps_per_epoch,\n    validation_steps=validation_steps,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-02T20:58:57.659208Z","iopub.execute_input":"2025-02-02T20:58:57.659558Z","iopub.status.idle":"2025-02-02T21:09:39.962502Z","shell.execute_reply.started":"2025-02-02T20:58:57.65953Z","shell.execute_reply":"2025-02-02T21:09:39.96175Z"},"id":"9EWOe9pbNePQ","outputId":"9a07d80e-00f8-4a2c-e8ab-0be6065ff3e1","collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Plot training history\nplt.figure(figsize=(12, 4))\n\nplt.subplot(1, 2, 1)\nplt.plot(history.history['loss'], label='Training Loss')\nplt.plot(history.history['val_loss'], label='Validation Loss')\nplt.title('Model Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.subplot(1, 2, 2)\nplt.plot(history.history['multiclass_iou_metric'], label='Training IoU')\nplt.plot(history.history['val_multiclass_iou_metric'], label='Validation IoU')\nplt.title('Model IoU')\nplt.xlabel('Epoch')\nplt.ylabel('IoU')\nplt.legend()\n\nplt.tight_layout()\nplt.show()\ntest_dataset=load_dataset(test_files)\n\ntest_results = multiClassWorkingModel.evaluate(test_dataset, steps=test_steps)\nprint(f\"\\nTest Loss: {test_results[0]:.4f}\")\nprint(f\"Test IoU: {test_results[1]:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-02T21:09:51.142237Z","iopub.execute_input":"2025-02-02T21:09:51.14258Z","iopub.status.idle":"2025-02-02T21:09:53.373185Z","shell.execute_reply.started":"2025-02-02T21:09:51.142552Z","shell.execute_reply":"2025-02-02T21:09:53.372518Z"},"id":"yr51bL7_NePR","outputId":"52379436-5b96-4e02-9b6b-3886501c021f","collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"INJURY_CLASSES = {\n    0: {\"name\": \"background\", \"color\": (0, 0, 0)},      # Black for background\n    1: {\"name\": \"abrasion\", \"color\": (255, 0, 0)},      # Red\n    2: {\"name\": \"bruise\", \"color\": (0, 0, 255)},        # Blue\n    3: {\"name\": \"cut\", \"color\": (0, 255, 0)},           # Green\n    4: {\"name\": \"laceration\", \"color\": (255, 255, 0)},  # Yellow\n    5: {\"name\": \"stab_wound\", \"color\": (255, 0, 255)}   # Magenta\n}\n\n# [Previous code remains the same up until the visualization functions]\n\ndef create_overlay_mask(mask, alpha=0.4):\n    \"\"\"\n    Create a colored overlay visualization with mutually exclusive classes\n    \"\"\"\n    # Remove batch dimension if present\n    if len(mask.shape) > 3:\n        mask = mask[0]\n\n    height, width = mask.shape[:2]\n    overlay = np.zeros((height, width, 4), dtype=np.float32)\n\n    # Get the class with highest probability for each pixel\n    class_indices = np.argmax(mask, axis=-1)\n    \n    # Create colored overlay using winning class only\n    for class_idx in range(NUM_CLASSES):\n        class_mask = (class_indices == class_idx)\n        if np.any(class_mask):\n            color = INJURY_CLASSES[class_idx][\"color\"]\n            overlay[class_mask, 0] = color[0] / 255.0\n            overlay[class_mask, 1] = color[1] / 255.0\n            overlay[class_mask, 2] = color[2] / 255.0\n            overlay[class_mask, 3] = alpha\n\n    return overlay\n\ndef visualize_prediction(model, dataset):\n    \"\"\"\n    Visualize predictions with mutually exclusive class assignment\n    \"\"\"\n    for image, true_mask in dataset.take(10):\n        if isinstance(image, tf.Tensor):\n            image = image.numpy()\n        if isinstance(true_mask, tf.Tensor):\n            true_mask = true_mask.numpy()\n\n        # Make prediction\n        pred_mask = model.predict(image[0:1])\n        display_image = image[0]\n\n        # Create overlays\n        true_overlay = create_overlay_mask(true_mask)\n        pred_overlay = create_overlay_mask(pred_mask)\n\n        # Create figure\n        plt.figure(figsize=(20, 7))\n\n        # Plot original image\n        plt.subplot(1, 3, 1)\n        plt.imshow(display_image)\n        plt.title(\"Original Image\")\n        plt.axis('off')\n\n        # Plot true segmentation\n        plt.subplot(1, 3, 2)\n        plt.imshow(display_image)\n        plt.imshow(true_overlay)\n        plt.title(\"True Segmentation\")\n        plt.axis('off')\n\n        # Plot predicted segmentation\n        plt.subplot(1, 3, 3)\n        plt.imshow(display_image)\n        plt.imshow(pred_overlay)\n        plt.title(\"Predicted Segmentation\")\n        plt.axis('off')\n\n        # Add legend\n        legend_elements = [\n            plt.Rectangle((0,0), 1, 1,\n                         fc=tuple(c/255 for c in INJURY_CLASSES[i][\"color\"]) + (0.6,),\n                         label=INJURY_CLASSES[i][\"name\"])\n            for i in range(NUM_CLASSES)\n        ]\n        plt.figlegend(handles=legend_elements,\n                     loc='center right',\n                     bbox_to_anchor=(0.98, 0.5),\n                     title=\"Injury Types\")\n\n        plt.tight_layout()\n        plt.show()\n\n        # Print statistics with mutually exclusive classification\n        print(\"\\nDetected injuries:\")\n        pred_classes = np.argmax(pred_mask[0], axis=-1)\n        true_classes = np.argmax(true_mask[0], axis=-1)\n        \n        for i in range(NUM_CLASSES):\n            true_pixels = np.sum(true_classes == i)\n            pred_pixels = np.sum(pred_classes == i)\n            if true_pixels > 0 or pred_pixels > 0:\n                print(f\"{INJURY_CLASSES[i]['name']}:\")\n                print(f\"  True pixels: {true_pixels}\")\n                print(f\"  Predicted pixels: {pred_pixels}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-02T21:10:44.762933Z","iopub.execute_input":"2025-02-02T21:10:44.763245Z","iopub.status.idle":"2025-02-02T21:10:44.776062Z","shell.execute_reply.started":"2025-02-02T21:10:44.763219Z","shell.execute_reply":"2025-02-02T21:10:44.775055Z"},"id":"6yC-OHGSNePR"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"visualize_prediction(multiClassWorkingModel,train_dataset)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-02T21:10:47.448788Z","iopub.execute_input":"2025-02-02T21:10:47.449105Z","iopub.status.idle":"2025-02-02T21:11:00.753419Z","shell.execute_reply.started":"2025-02-02T21:10:47.449078Z","shell.execute_reply":"2025-02-02T21:11:00.752532Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"id":"zytXkgWJNePS","outputId":"26dec837-e627-4dd5-add9-5d474bda50de"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## END\n","metadata":{"id":"X9Px_FslNePS"}},{"cell_type":"markdown","source":"# WORKING MULTICLASS SEGMENTATION END\n","metadata":{"id":"nIQWO6_sNePS"}},{"cell_type":"code","source":"# Train model\nhistory = multiClassWorkingModel.fit(\n    train_dataset,\n    validation_data=val_dataset,\n    epochs=EPOCHS,\n    steps_per_epoch=steps_per_epoch,\n    validation_steps=validation_steps,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-02T21:13:43.834036Z","iopub.execute_input":"2025-02-02T21:13:43.834344Z","iopub.status.idle":"2025-02-02T21:24:01.609418Z","shell.execute_reply.started":"2025-02-02T21:13:43.83432Z","shell.execute_reply":"2025-02-02T21:24:01.608706Z"},"id":"NUfgzj9xNePS","outputId":"5856020a-35c9-4eea-9f67-052a7f37adf3"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"multiClassWorkingModel.fit(\n    train_dataset,\n    validation_data=val_dataset,\n    epochs=5,\n    steps_per_epoch=steps_per_epoch,\n    validation_steps=validation_steps,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-02T21:26:14.104015Z","iopub.execute_input":"2025-02-02T21:26:14.104345Z","iopub.status.idle":"2025-02-02T21:36:33.553106Z","shell.execute_reply.started":"2025-02-02T21:26:14.10432Z","shell.execute_reply":"2025-02-02T21:36:33.552323Z"},"id":"PVE-Mf22NePT","outputId":"2ca5ee7d-f274-4809-b72c-a07bc042ae8b"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"multiClassWorkingModel.fit(\n    train_dataset,\n    validation_data=val_dataset,\n    epochs=5,\n    steps_per_epoch=steps_per_epoch,\n    validation_steps=validation_steps,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-02T21:52:04.431468Z","iopub.execute_input":"2025-02-02T21:52:04.43182Z","iopub.status.idle":"2025-02-02T22:02:22.176919Z","shell.execute_reply.started":"2025-02-02T21:52:04.431795Z","shell.execute_reply":"2025-02-02T22:02:22.176084Z"},"id":"qqs-zFC1NePT","outputId":"3e6ed7cd-551e-42a6-b2d6-995bca4768cc","collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"visualize_prediction(multiClassWorkingModel,test_dataset)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-02T22:04:19.761937Z","iopub.execute_input":"2025-02-02T22:04:19.76227Z","iopub.status.idle":"2025-02-02T22:04:30.196428Z","shell.execute_reply.started":"2025-02-02T22:04:19.762245Z","shell.execute_reply":"2025-02-02T22:04:30.195018Z"},"id":"OhBwhJFINePU","outputId":"34faf73c-135d-4a71-db83-a595cefa31af","collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_results = multiClassWorkingModel.evaluate(test_dataset, steps=test_steps)\nprint(f\"\\nTest Loss: {test_results[0]:.4f}\")\nprint(f\"Test IoU: {test_results[1]:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-02T22:06:03.311983Z","iopub.execute_input":"2025-02-02T22:06:03.312314Z","iopub.status.idle":"2025-02-02T22:06:05.125035Z","shell.execute_reply.started":"2025-02-02T22:06:03.312289Z","shell.execute_reply":"2025-02-02T22:06:05.12431Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"multiClassWorkingModel.fit(\n    train_dataset,\n    validation_data=val_dataset,\n    epochs=5,\n    steps_per_epoch=steps_per_epoch,\n    validation_steps=validation_steps,\n)\ntest_results = multiClassWorkingModel.evaluate(test_dataset, steps=test_steps)\nprint(f\"\\nTest Loss: {test_results[0]:.4f}\")\nprint(f\"Test IoU: {test_results[1]:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-02T22:08:23.280636Z","iopub.execute_input":"2025-02-02T22:08:23.280966Z","iopub.status.idle":"2025-02-02T22:18:44.101258Z","shell.execute_reply.started":"2025-02-02T22:08:23.280945Z","shell.execute_reply":"2025-02-02T22:18:44.100573Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"multiClassWorkingModel.fit(\n    train_dataset,\n    validation_data=val_dataset,\n    epochs=5,\n    steps_per_epoch=steps_per_epoch,\n    validation_steps=validation_steps,\n)\ntest_results = multiClassWorkingModel.evaluate(test_dataset, steps=test_steps)\nprint(f\"\\nTest Loss: {test_results[0]:.4f}\")\nprint(f\"Test IoU: {test_results[1]:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-02T22:18:44.102373Z","iopub.execute_input":"2025-02-02T22:18:44.102667Z","iopub.status.idle":"2025-02-02T22:29:01.673241Z","shell.execute_reply.started":"2025-02-02T22:18:44.102644Z","shell.execute_reply":"2025-02-02T22:29:01.672291Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"multiClassWorkingModel.fit(\n    train_dataset,\n    validation_data=val_dataset,\n    epochs=5,\n    steps_per_epoch=steps_per_epoch,\n    validation_steps=validation_steps,\n)\ntest_results = multiClassWorkingModel.evaluate(test_dataset, steps=test_steps)\nprint(f\"\\nTest Loss: {test_results[0]:.4f}\")\nprint(f\"Test IoU: {test_results[1]:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-02T22:29:01.674451Z","iopub.execute_input":"2025-02-02T22:29:01.674745Z","iopub.status.idle":"2025-02-02T22:39:18.79303Z","shell.execute_reply.started":"2025-02-02T22:29:01.674721Z","shell.execute_reply":"2025-02-02T22:39:18.792248Z"}},"outputs":[],"execution_count":null}]}